{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import os\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import OpenAI\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModel, BitsAndBytesConfig\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from bangla_pdf_ocr import process_pdf\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "import torch, re\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.is_available())  \n",
    "# print(torch.cuda.get_device_name(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets Read the document\n",
    "# def read_doc(directory):\n",
    "#     file_loader=PyPDFDirectoryLoader(directory)\n",
    "#     documents=file_loader.load()\n",
    "#     return documents\n",
    "\n",
    "# doc=read_doc('data/')\n",
    "# len(doc)\n",
    "# type(doc)\n",
    "# print(doc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/HSC26-Bangla1st-Paper.pdf\"\n",
    "output_file = \"data/cleaned_data.txt\"\n",
    "process_pdf(path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = DirectoryLoader(\"data\", glob=\"**/*.txt\")\n",
    "doc= doc.load()\n",
    "type(doc)\n",
    "#print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean each document\n",
    "for i in range(len(doc)):\n",
    "    content = doc[i].page_content\n",
    "    match = re.search(r'Page 6(.*?)Page 20', content, re.DOTALL)\n",
    "    content = match.group(1)\n",
    "    # Remove English letters, digits, and hyphens\n",
    "    cleaned = re.sub(r'[a-zA-Z0-9\\-\\\\n]', '', content)\n",
    "    # Replace multiple whitespace with a single space or newline\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    doc[i].page_content = cleaned.strip()\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide the docs into chunks\n",
    "def chunk_data(docs, chunk_size=1000, chunk_overlap=100):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    doc=text_splitter.split_documents(docs)\n",
    "    return doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=chunk_data(docs=doc)\n",
    "len(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz  \n",
    "\n",
    "# def extract_bangla_text_from_pdf(pdf_path):\n",
    "#     doc = fitz.open(pdf_path)\n",
    "#     text = \"\"\n",
    "#     for page in doc:\n",
    "#         text += page.get_text()\n",
    "#     return text\n",
    "\n",
    "# text = extract_bangla_text_from_pdf(\"data/HSC26-Bangla1st-Paper.pdf\")\n",
    "\n",
    "# def clean_bangla_text(text):\n",
    "#     import re\n",
    "#     text = re.sub(r'\\s+', ' ', text)\n",
    "#     text = text.strip()\n",
    "#     return text\n",
    "\n",
    "# text = clean_bangla_text(text)\n",
    "# print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding Technique Of OPENAI\n",
    "# embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=os.environ['OPENAI_API_KEY'])\n",
    "# embeddings\n",
    "# model = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")  # supports Bangla\n",
    "\n",
    "# embeddings = model.encode([text])\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"l3cube-pune/bengali-sentence-similarity-sbert\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors=embeddings.embed_query(\"How are you?\")\n",
    "# len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Vector Search DB In Pinecone\n",
    "# pc = pinecone.Pinecone(api_key='pcsk_6aVRrV_442vekbbyorbm5i64i2U4Uk5vYC4Bvng2zRXj4HXoKyfajA5XsnT2NC7Z2KgAfL')\n",
    "# index_name = pc.Index(\"10mins\")\n",
    "# PineconeVectorStore.from_documents(doc, embeddings, index_name=\"10mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pinecone.Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))  \n",
    "\n",
    "index_name = \"10mins\"\n",
    "# Check if index already exists\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # Create index using ServerlessSpec\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        vector_type = \"dense\",\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=pinecone.ServerlessSpec(\n",
    "            cloud=\"aws\", \n",
    "            region=\"us-east-1\"  \n",
    "        )\n",
    "    )\n",
    "\n",
    "    #PineconeVectorStore.from_documents(doc, embeddings, index_name=\"10mins\")\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "# Only run this once!\n",
    "if index.describe_index_stats()[\"total_vector_count\"] == 0:\n",
    "    PineconeVectorStore.from_documents(doc, embeddings, index_name=index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0,\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"BanglaLLM/bangla-llama-13b-base-v0.1\")\n",
    "# llm = AutoModelForCausalLM.from_pretrained(\"BanglaLLM/bangla-llama-13b-base-v0.1\")\n",
    "\n",
    "#llm = AutoModelForCausalLM.from_pretrained(\"BanglaLLM/bangla-llama-13b-base-v0.1\")\n",
    "\n",
    "# Wrap existing Pinecone index for LangChain\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index,             \n",
    "    embedding=embeddings, \n",
    "    text_key=\"text\"\n",
    ")\n",
    "\n",
    "\n",
    "# Short-answer prompt for individual chunks (map step)\n",
    "question_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful assistant. Answer the following question based on the context below.\n",
    "Keep your answer very short — a single line only.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Short-answer prompt for final summarization (reduce step)\n",
    "combine_prompt = PromptTemplate(\n",
    "    input_variables=[\"summaries\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Based on the following partial answers, generate a single short one-line answer.\n",
    "\n",
    "Answers:\n",
    "{summaries}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10}),  \n",
    "    chain_type= \"map_reduce\",\n",
    "    chain_type_kwargs={\n",
    "        \"question_prompt\": question_prompt,\n",
    "        \"combine_prompt\": combine_prompt\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cosine Similarity Retreive Results from VectorDB\n",
    "# def retrieve_query(query, k=1):\n",
    "#     matching_results=vectorstore.similarity_search(query,k=k)\n",
    "#     return matching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load tokenizer and model (after conversion)\n",
    "# llm = AutoModel.from_pretrained(\"asif00/bangla-llama-1B-gguf-16bit\", torch_dtype=\"auto\"),\n",
    "\n",
    "# chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     retriever=index.as_retriever(),  \n",
    "#     chain_type=\"map_reduce\",\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search answers from VectorDB\n",
    "def retrieve_answers(query):\n",
    "    # doc_search=retrieve_query(query)\n",
    "    # print(doc_search)\n",
    "    response=chain.invoke({\"query\": query})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_query = \"বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\"\n",
    "answer = retrieve_answers(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_query = \"অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\"\n",
    "answer = retrieve_answers(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_query = \"কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?\"\n",
    "answer = retrieve_answers(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
